{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv(r'C:/Users/acer/r/Downloads/MessagePolarity_ParticipantsData/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample=pd.read_csv(r'C:/Users/acer/Downloads/MessagePolarity_ParticipantsData/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <th>StylizedLetters</th>\n",
       "      <th>IsGoodNews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>2.620660</td>\n",
       "      <td>1.253645</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>0.157385</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>0.222453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>3.837751</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.195476</td>\n",
       "      <td>-0.408024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.187634</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.210190</td>\n",
       "      <td>2.682528</td>\n",
       "      <td>1.291868</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>0.221744</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.859101</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>2.374782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.340723</td>\n",
       "      <td>1.334201</td>\n",
       "      <td>2.270899</td>\n",
       "      <td>0.602985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.113133</td>\n",
       "      <td>-0.128592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  Freq_Of_Word_4  \\\n",
       "0       -0.351864        2.620660        1.253645       -0.039223   \n",
       "1       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "2       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "3        1.210190        2.682528        1.291868       -0.039223   \n",
       "4       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "\n",
       "   Freq_Of_Word_5  Freq_Of_Word_6  Freq_Of_Word_7  Freq_Of_Word_8  \\\n",
       "0       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "1       -0.465210       -0.353977       -0.304257        3.837751   \n",
       "2       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "3        0.221744       -0.353977       -0.304257        0.859101   \n",
       "4       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "\n",
       "   Freq_Of_Word_9  Freq_Of_Word_10  ...  Freq_Of_Word_45  Freq_Of_Word_46  \\\n",
       "0       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "1       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "2       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "3       -0.318797         2.374782  ...        -0.323019        -0.205212   \n",
       "4       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "\n",
       "   Freq_Of_Word_47  Freq_Of_Word_48  Freq_Of_Word_49  Freq_Of_Word_50  \\\n",
       "0        -0.079531        -0.118688         0.079303         0.157385   \n",
       "1        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "2        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "3        -0.079531        -0.118688        -0.151911         0.340723   \n",
       "4        -0.079531        -0.118688        -0.151911         0.930461   \n",
       "\n",
       "   TotalEmojiCharacters  LengthOFFirstParagraph  StylizedLetters  IsGoodNews  \n",
       "0             -0.028751               -0.046474         0.222453           1  \n",
       "1             -0.107383               -0.195476        -0.408024           0  \n",
       "2             -0.107383               -0.187634        -0.392578           0  \n",
       "3              1.334201                2.270899         0.602985           1  \n",
       "4             -0.028751               -0.113133        -0.128592           0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    579\n",
       "1    368\n",
       "Name: IsGoodNews, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.IsGoodNews.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freq_Of_Word_1            0\n",
       "Freq_Of_Word_2            0\n",
       "Freq_Of_Word_3            0\n",
       "Freq_Of_Word_4            0\n",
       "Freq_Of_Word_5            0\n",
       "Freq_Of_Word_6            0\n",
       "Freq_Of_Word_7            0\n",
       "Freq_Of_Word_8            0\n",
       "Freq_Of_Word_9            0\n",
       "Freq_Of_Word_10           0\n",
       "Freq_Of_Word_11           0\n",
       "Freq_Of_Word_12           0\n",
       "Freq_Of_Word_13           0\n",
       "Freq_Of_Word_14           0\n",
       "Freq_Of_Word_15           0\n",
       "Freq_Of_Word_16           0\n",
       "Freq_Of_Word_17           0\n",
       "Freq_Of_Word_18           0\n",
       "Freq_Of_Word_19           0\n",
       "Freq_Of_Word_20           0\n",
       "Freq_Of_Word_21           0\n",
       "Freq_Of_Word_22           0\n",
       "Freq_Of_Word_23           0\n",
       "Freq_Of_Word_24           0\n",
       "Freq_Of_Word_25           0\n",
       "Freq_Of_Word_26           0\n",
       "Freq_Of_Word_27           0\n",
       "Freq_Of_Word_28           0\n",
       "Freq_Of_Word_29           0\n",
       "Freq_Of_Word_30           0\n",
       "Freq_Of_Word_31           0\n",
       "Freq_Of_Word_32           0\n",
       "Freq_Of_Word_33           0\n",
       "Freq_Of_Word_34           0\n",
       "Freq_Of_Word_35           0\n",
       "Freq_Of_Word_36           0\n",
       "Freq_Of_Word_37           0\n",
       "Freq_Of_Word_38           0\n",
       "Freq_Of_Word_39           0\n",
       "Freq_Of_Word_40           0\n",
       "Freq_Of_Word_41           0\n",
       "Freq_Of_Word_42           0\n",
       "Freq_Of_Word_43           0\n",
       "Freq_Of_Word_44           0\n",
       "Freq_Of_Word_45           0\n",
       "Freq_Of_Word_46           0\n",
       "Freq_Of_Word_47           0\n",
       "Freq_Of_Word_48           0\n",
       "Freq_Of_Word_49           0\n",
       "Freq_Of_Word_50           0\n",
       "TotalEmojiCharacters      0\n",
       "LengthOFFirstParagraph    0\n",
       "StylizedLetters           0\n",
       "IsGoodNews                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X.iloc[:,:-1]\n",
    "Y_train=X.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 53)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527, 53)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <th>StylizedLetters</th>\n",
       "      <th>IsGoodNews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>-0.002367</td>\n",
       "      <td>-0.013153</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.081520</td>\n",
       "      <td>0.047176</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.096471</td>\n",
       "      <td>0.048763</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036678</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>-0.019728</td>\n",
       "      <td>-0.004575</td>\n",
       "      <td>-0.038761</td>\n",
       "      <td>-0.013797</td>\n",
       "      <td>0.008485</td>\n",
       "      <td>0.067895</td>\n",
       "      <td>0.053014</td>\n",
       "      <td>0.118213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <td>0.010394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>0.172478</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.043161</td>\n",
       "      <td>0.210548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027928</td>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.042215</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>-0.058462</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>0.148034</td>\n",
       "      <td>0.047688</td>\n",
       "      <td>0.170886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <td>-0.002367</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021998</td>\n",
       "      <td>0.130316</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>0.010934</td>\n",
       "      <td>0.062003</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085817</td>\n",
       "      <td>-0.075904</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>-0.031101</td>\n",
       "      <td>-0.045449</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.032936</td>\n",
       "      <td>0.091458</td>\n",
       "      <td>0.024856</td>\n",
       "      <td>0.167442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <td>-0.013153</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>-0.021998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016472</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.032053</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012720</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>-0.002874</td>\n",
       "      <td>-0.005553</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.046516</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>0.048563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <td>0.020966</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.130316</td>\n",
       "      <td>-0.016472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064095</td>\n",
       "      <td>0.100868</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057029</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>-0.027709</td>\n",
       "      <td>-0.054150</td>\n",
       "      <td>-0.042927</td>\n",
       "      <td>-0.082370</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.038279</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>0.231699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <td>0.081520</td>\n",
       "      <td>-0.003339</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>-0.014258</td>\n",
       "      <td>0.064095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089194</td>\n",
       "      <td>0.064522</td>\n",
       "      <td>0.081660</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082128</td>\n",
       "      <td>-0.059036</td>\n",
       "      <td>-0.024670</td>\n",
       "      <td>-0.045313</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.010733</td>\n",
       "      <td>0.126238</td>\n",
       "      <td>0.051525</td>\n",
       "      <td>0.259183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <td>0.047176</td>\n",
       "      <td>0.172478</td>\n",
       "      <td>-0.011462</td>\n",
       "      <td>0.032053</td>\n",
       "      <td>0.100868</td>\n",
       "      <td>0.089194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>0.106784</td>\n",
       "      <td>0.083965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066613</td>\n",
       "      <td>-0.068576</td>\n",
       "      <td>-0.025366</td>\n",
       "      <td>-0.044060</td>\n",
       "      <td>-0.036877</td>\n",
       "      <td>-0.073792</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>-0.032189</td>\n",
       "      <td>0.394999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.010934</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.064522</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110751</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056538</td>\n",
       "      <td>-0.042613</td>\n",
       "      <td>-0.015039</td>\n",
       "      <td>-0.011828</td>\n",
       "      <td>-0.028695</td>\n",
       "      <td>-0.062708</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.072371</td>\n",
       "      <td>0.034428</td>\n",
       "      <td>0.181393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <td>0.096471</td>\n",
       "      <td>0.043161</td>\n",
       "      <td>0.062003</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.081660</td>\n",
       "      <td>0.106784</td>\n",
       "      <td>0.110751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>-0.052799</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.028991</td>\n",
       "      <td>-0.055444</td>\n",
       "      <td>0.065809</td>\n",
       "      <td>0.248130</td>\n",
       "      <td>0.194155</td>\n",
       "      <td>0.226195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <td>0.048763</td>\n",
       "      <td>0.210548</td>\n",
       "      <td>0.023857</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.008806</td>\n",
       "      <td>0.083965</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.107459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044976</td>\n",
       "      <td>-0.043607</td>\n",
       "      <td>-0.020726</td>\n",
       "      <td>-0.019523</td>\n",
       "      <td>-0.007730</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.054715</td>\n",
       "      <td>0.092510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_11</th>\n",
       "      <td>0.045993</td>\n",
       "      <td>0.154969</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>-0.012150</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>0.337929</td>\n",
       "      <td>0.152854</td>\n",
       "      <td>0.144659</td>\n",
       "      <td>0.143873</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090731</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>-0.017508</td>\n",
       "      <td>-0.033603</td>\n",
       "      <td>-0.038619</td>\n",
       "      <td>-0.088521</td>\n",
       "      <td>0.057045</td>\n",
       "      <td>0.147225</td>\n",
       "      <td>0.097364</td>\n",
       "      <td>0.305149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_12</th>\n",
       "      <td>0.104085</td>\n",
       "      <td>0.051260</td>\n",
       "      <td>0.054657</td>\n",
       "      <td>-0.007582</td>\n",
       "      <td>0.094738</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>-0.018704</td>\n",
       "      <td>-0.016754</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.082312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102704</td>\n",
       "      <td>-0.070267</td>\n",
       "      <td>0.108387</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>-0.016121</td>\n",
       "      <td>-0.032481</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>-0.016894</td>\n",
       "      <td>-0.008034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_13</th>\n",
       "      <td>0.064742</td>\n",
       "      <td>0.034448</td>\n",
       "      <td>0.054648</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.149552</td>\n",
       "      <td>-0.030903</td>\n",
       "      <td>0.007588</td>\n",
       "      <td>0.064029</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083497</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>-0.014958</td>\n",
       "      <td>-0.039947</td>\n",
       "      <td>-0.089409</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>0.070099</td>\n",
       "      <td>0.098717</td>\n",
       "      <td>0.111134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_14</th>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.014103</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>-0.057257</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.047216</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>-0.025687</td>\n",
       "      <td>-0.009127</td>\n",
       "      <td>-0.026366</td>\n",
       "      <td>-0.023472</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.061695</td>\n",
       "      <td>0.120967</td>\n",
       "      <td>-0.002024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_15</th>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.106974</td>\n",
       "      <td>0.151548</td>\n",
       "      <td>0.054825</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>0.182595</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.068678</td>\n",
       "      <td>0.241813</td>\n",
       "      <td>0.126007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>-0.011531</td>\n",
       "      <td>-0.026295</td>\n",
       "      <td>-0.025377</td>\n",
       "      <td>-0.047241</td>\n",
       "      <td>0.033021</td>\n",
       "      <td>0.419893</td>\n",
       "      <td>0.175635</td>\n",
       "      <td>0.194164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_16</th>\n",
       "      <td>0.061790</td>\n",
       "      <td>0.117832</td>\n",
       "      <td>0.094623</td>\n",
       "      <td>-0.014719</td>\n",
       "      <td>0.080174</td>\n",
       "      <td>0.053130</td>\n",
       "      <td>0.194472</td>\n",
       "      <td>0.145406</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.023513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.068662</td>\n",
       "      <td>-0.027590</td>\n",
       "      <td>-0.036098</td>\n",
       "      <td>-0.049883</td>\n",
       "      <td>-0.073027</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.026770</td>\n",
       "      <td>-0.015521</td>\n",
       "      <td>0.346826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_17</th>\n",
       "      <td>0.053233</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.112351</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>0.172559</td>\n",
       "      <td>0.232003</td>\n",
       "      <td>0.284616</td>\n",
       "      <td>0.105277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075456</td>\n",
       "      <td>-0.064052</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.035204</td>\n",
       "      <td>-0.022935</td>\n",
       "      <td>-0.068534</td>\n",
       "      <td>0.053995</td>\n",
       "      <td>0.132124</td>\n",
       "      <td>0.042107</td>\n",
       "      <td>0.234857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_18</th>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.240146</td>\n",
       "      <td>0.105804</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.138829</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>0.066808</td>\n",
       "      <td>0.018174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066540</td>\n",
       "      <td>-0.045892</td>\n",
       "      <td>-0.025144</td>\n",
       "      <td>-0.026060</td>\n",
       "      <td>-0.043155</td>\n",
       "      <td>-0.060639</td>\n",
       "      <td>-0.010902</td>\n",
       "      <td>0.096371</td>\n",
       "      <td>0.027136</td>\n",
       "      <td>0.182224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_19</th>\n",
       "      <td>0.135201</td>\n",
       "      <td>0.127997</td>\n",
       "      <td>0.117618</td>\n",
       "      <td>0.018304</td>\n",
       "      <td>0.094023</td>\n",
       "      <td>0.083722</td>\n",
       "      <td>0.116231</td>\n",
       "      <td>-0.010299</td>\n",
       "      <td>0.037624</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084670</td>\n",
       "      <td>-0.004198</td>\n",
       "      <td>-0.055564</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>-0.049389</td>\n",
       "      <td>-0.167888</td>\n",
       "      <td>-0.030387</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>-0.033011</td>\n",
       "      <td>0.235022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_20</th>\n",
       "      <td>0.024064</td>\n",
       "      <td>-0.013738</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>-0.003336</td>\n",
       "      <td>0.033087</td>\n",
       "      <td>0.053960</td>\n",
       "      <td>0.084406</td>\n",
       "      <td>0.187753</td>\n",
       "      <td>0.195269</td>\n",
       "      <td>0.045806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054071</td>\n",
       "      <td>-0.042216</td>\n",
       "      <td>-0.004252</td>\n",
       "      <td>-0.029482</td>\n",
       "      <td>-0.021745</td>\n",
       "      <td>-0.055709</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>0.128650</td>\n",
       "      <td>0.075401</td>\n",
       "      <td>0.244805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_21</th>\n",
       "      <td>0.073889</td>\n",
       "      <td>0.178618</td>\n",
       "      <td>0.095158</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.144796</td>\n",
       "      <td>0.186967</td>\n",
       "      <td>0.198694</td>\n",
       "      <td>0.313332</td>\n",
       "      <td>0.181550</td>\n",
       "      <td>0.065709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008939</td>\n",
       "      <td>-0.094646</td>\n",
       "      <td>-0.046155</td>\n",
       "      <td>-0.067190</td>\n",
       "      <td>-0.061972</td>\n",
       "      <td>-0.111004</td>\n",
       "      <td>0.035218</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.026482</td>\n",
       "      <td>0.403225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_22</th>\n",
       "      <td>-0.033844</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>-0.044568</td>\n",
       "      <td>-0.005475</td>\n",
       "      <td>-0.040629</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>-0.028517</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>-0.021318</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041964</td>\n",
       "      <td>-0.029263</td>\n",
       "      <td>-0.010413</td>\n",
       "      <td>-0.020075</td>\n",
       "      <td>0.513042</td>\n",
       "      <td>-0.057050</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0.084280</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.094523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_23</th>\n",
       "      <td>0.188110</td>\n",
       "      <td>0.038223</td>\n",
       "      <td>0.161322</td>\n",
       "      <td>0.114027</td>\n",
       "      <td>0.089794</td>\n",
       "      <td>0.266449</td>\n",
       "      <td>0.123591</td>\n",
       "      <td>0.047161</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.073070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064023</td>\n",
       "      <td>-0.046672</td>\n",
       "      <td>-0.016891</td>\n",
       "      <td>-0.035599</td>\n",
       "      <td>-0.017824</td>\n",
       "      <td>-0.056141</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.262555</td>\n",
       "      <td>0.182349</td>\n",
       "      <td>0.325740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_24</th>\n",
       "      <td>0.233784</td>\n",
       "      <td>0.060765</td>\n",
       "      <td>0.089256</td>\n",
       "      <td>-0.012118</td>\n",
       "      <td>0.068485</td>\n",
       "      <td>0.142167</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.069177</td>\n",
       "      <td>0.193540</td>\n",
       "      <td>0.082154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072353</td>\n",
       "      <td>-0.048612</td>\n",
       "      <td>-0.010971</td>\n",
       "      <td>-0.045024</td>\n",
       "      <td>-0.034123</td>\n",
       "      <td>-0.068233</td>\n",
       "      <td>0.063139</td>\n",
       "      <td>0.227742</td>\n",
       "      <td>0.193163</td>\n",
       "      <td>0.359695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_25</th>\n",
       "      <td>-0.066760</td>\n",
       "      <td>-0.045073</td>\n",
       "      <td>-0.090321</td>\n",
       "      <td>-0.012576</td>\n",
       "      <td>-0.104150</td>\n",
       "      <td>-0.094234</td>\n",
       "      <td>-0.098229</td>\n",
       "      <td>-0.049856</td>\n",
       "      <td>-0.077909</td>\n",
       "      <td>-0.046410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099259</td>\n",
       "      <td>-0.039051</td>\n",
       "      <td>-0.011618</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.057537</td>\n",
       "      <td>0.150820</td>\n",
       "      <td>-0.019093</td>\n",
       "      <td>-0.068274</td>\n",
       "      <td>-0.041265</td>\n",
       "      <td>-0.251108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_26</th>\n",
       "      <td>-0.051950</td>\n",
       "      <td>-0.026722</td>\n",
       "      <td>-0.070649</td>\n",
       "      <td>-0.011417</td>\n",
       "      <td>-0.094840</td>\n",
       "      <td>-0.096180</td>\n",
       "      <td>-0.085849</td>\n",
       "      <td>-0.047542</td>\n",
       "      <td>-0.063494</td>\n",
       "      <td>-0.014577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032278</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>0.023479</td>\n",
       "      <td>0.007272</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.109468</td>\n",
       "      <td>-0.018088</td>\n",
       "      <td>-0.065806</td>\n",
       "      <td>-0.054896</td>\n",
       "      <td>-0.226468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_27</th>\n",
       "      <td>-0.065578</td>\n",
       "      <td>-0.038330</td>\n",
       "      <td>-0.072454</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.072034</td>\n",
       "      <td>-0.039960</td>\n",
       "      <td>-0.076389</td>\n",
       "      <td>-0.047792</td>\n",
       "      <td>-0.065843</td>\n",
       "      <td>-0.041037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075348</td>\n",
       "      <td>-0.036627</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>0.045517</td>\n",
       "      <td>-0.023548</td>\n",
       "      <td>0.052358</td>\n",
       "      <td>-0.018701</td>\n",
       "      <td>-0.073944</td>\n",
       "      <td>-0.086898</td>\n",
       "      <td>-0.180674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_28</th>\n",
       "      <td>-0.059961</td>\n",
       "      <td>-0.045148</td>\n",
       "      <td>-0.039450</td>\n",
       "      <td>-0.008664</td>\n",
       "      <td>-0.078768</td>\n",
       "      <td>-0.071675</td>\n",
       "      <td>-0.075435</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.050080</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022780</td>\n",
       "      <td>-0.041392</td>\n",
       "      <td>-0.016480</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>-0.029745</td>\n",
       "      <td>0.326506</td>\n",
       "      <td>-0.010961</td>\n",
       "      <td>-0.049216</td>\n",
       "      <td>-0.064887</td>\n",
       "      <td>-0.129274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_29</th>\n",
       "      <td>-0.044040</td>\n",
       "      <td>-0.038957</td>\n",
       "      <td>-0.040833</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>0.170280</td>\n",
       "      <td>-0.051692</td>\n",
       "      <td>-0.061369</td>\n",
       "      <td>-0.033022</td>\n",
       "      <td>-0.049772</td>\n",
       "      <td>-0.023312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005481</td>\n",
       "      <td>-0.033545</td>\n",
       "      <td>-0.013212</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.024418</td>\n",
       "      <td>0.148778</td>\n",
       "      <td>-0.013070</td>\n",
       "      <td>-0.054105</td>\n",
       "      <td>-0.059172</td>\n",
       "      <td>-0.142238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_30</th>\n",
       "      <td>-0.052091</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>-0.023884</td>\n",
       "      <td>-0.009199</td>\n",
       "      <td>-0.086614</td>\n",
       "      <td>-0.055782</td>\n",
       "      <td>-0.071875</td>\n",
       "      <td>-0.035481</td>\n",
       "      <td>-0.054701</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021652</td>\n",
       "      <td>-0.033445</td>\n",
       "      <td>-0.017496</td>\n",
       "      <td>-0.011558</td>\n",
       "      <td>-0.017259</td>\n",
       "      <td>0.208994</td>\n",
       "      <td>-0.014952</td>\n",
       "      <td>-0.058283</td>\n",
       "      <td>-0.065686</td>\n",
       "      <td>-0.187425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_31</th>\n",
       "      <td>-0.043987</td>\n",
       "      <td>-0.026008</td>\n",
       "      <td>-0.050324</td>\n",
       "      <td>-0.006765</td>\n",
       "      <td>-0.066002</td>\n",
       "      <td>-0.047805</td>\n",
       "      <td>-0.059636</td>\n",
       "      <td>-0.035443</td>\n",
       "      <td>-0.044865</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>-0.012868</td>\n",
       "      <td>-0.023175</td>\n",
       "      <td>-0.020359</td>\n",
       "      <td>0.268670</td>\n",
       "      <td>-0.009690</td>\n",
       "      <td>-0.041002</td>\n",
       "      <td>-0.048894</td>\n",
       "      <td>-0.138745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_32</th>\n",
       "      <td>-0.031670</td>\n",
       "      <td>-0.012993</td>\n",
       "      <td>-0.058553</td>\n",
       "      <td>-0.006173</td>\n",
       "      <td>-0.052345</td>\n",
       "      <td>-0.027583</td>\n",
       "      <td>-0.054480</td>\n",
       "      <td>-0.032307</td>\n",
       "      <td>-0.040725</td>\n",
       "      <td>-0.006383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>-0.026552</td>\n",
       "      <td>-0.011742</td>\n",
       "      <td>-0.022683</td>\n",
       "      <td>-0.013075</td>\n",
       "      <td>0.285373</td>\n",
       "      <td>-0.009745</td>\n",
       "      <td>-0.044899</td>\n",
       "      <td>-0.050110</td>\n",
       "      <td>-0.123148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_33</th>\n",
       "      <td>-0.046447</td>\n",
       "      <td>-0.044344</td>\n",
       "      <td>-0.073300</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>-0.049696</td>\n",
       "      <td>-0.051187</td>\n",
       "      <td>-0.037426</td>\n",
       "      <td>-0.033709</td>\n",
       "      <td>-0.028231</td>\n",
       "      <td>-0.044143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>0.036903</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>-0.011216</td>\n",
       "      <td>0.055633</td>\n",
       "      <td>-0.012850</td>\n",
       "      <td>-0.036991</td>\n",
       "      <td>-0.008380</td>\n",
       "      <td>-0.116501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_34</th>\n",
       "      <td>-0.031645</td>\n",
       "      <td>-0.017524</td>\n",
       "      <td>-0.061238</td>\n",
       "      <td>-0.006173</td>\n",
       "      <td>-0.051212</td>\n",
       "      <td>-0.027228</td>\n",
       "      <td>-0.054472</td>\n",
       "      <td>-0.032300</td>\n",
       "      <td>-0.040712</td>\n",
       "      <td>-0.006342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>-0.026542</td>\n",
       "      <td>-0.011740</td>\n",
       "      <td>-0.022679</td>\n",
       "      <td>-0.013402</td>\n",
       "      <td>0.285487</td>\n",
       "      <td>-0.009329</td>\n",
       "      <td>-0.042076</td>\n",
       "      <td>-0.044117</td>\n",
       "      <td>-0.125556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_35</th>\n",
       "      <td>-0.055118</td>\n",
       "      <td>-0.029811</td>\n",
       "      <td>-0.061394</td>\n",
       "      <td>-0.008994</td>\n",
       "      <td>-0.075444</td>\n",
       "      <td>-0.068704</td>\n",
       "      <td>-0.068418</td>\n",
       "      <td>-0.031703</td>\n",
       "      <td>-0.037520</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022133</td>\n",
       "      <td>-0.041579</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>-0.029379</td>\n",
       "      <td>0.202351</td>\n",
       "      <td>-0.009562</td>\n",
       "      <td>-0.040038</td>\n",
       "      <td>-0.046255</td>\n",
       "      <td>-0.168817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_36</th>\n",
       "      <td>-0.047682</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.044662</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.080465</td>\n",
       "      <td>-0.058658</td>\n",
       "      <td>-0.068495</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>-0.051631</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014576</td>\n",
       "      <td>-0.047337</td>\n",
       "      <td>0.048219</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>-0.030069</td>\n",
       "      <td>0.236874</td>\n",
       "      <td>-0.017428</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>-0.048603</td>\n",
       "      <td>-0.145174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_37</th>\n",
       "      <td>-0.066333</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>-0.076767</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>-0.091039</td>\n",
       "      <td>-0.100688</td>\n",
       "      <td>-0.035034</td>\n",
       "      <td>-0.018174</td>\n",
       "      <td>-0.035020</td>\n",
       "      <td>-0.015782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005131</td>\n",
       "      <td>0.176820</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.126325</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>-0.040567</td>\n",
       "      <td>-0.008354</td>\n",
       "      <td>-0.166855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_38</th>\n",
       "      <td>-0.018651</td>\n",
       "      <td>-0.014217</td>\n",
       "      <td>0.143998</td>\n",
       "      <td>-0.002481</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>-0.022917</td>\n",
       "      <td>-0.014861</td>\n",
       "      <td>-0.004863</td>\n",
       "      <td>-0.004502</td>\n",
       "      <td>-0.016683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>-0.004682</td>\n",
       "      <td>-0.001800</td>\n",
       "      <td>-0.008184</td>\n",
       "      <td>-0.002331</td>\n",
       "      <td>-0.005105</td>\n",
       "      <td>-0.019169</td>\n",
       "      <td>-0.017020</td>\n",
       "      <td>-0.038689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_39</th>\n",
       "      <td>-0.014406</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>-0.037474</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>-0.044715</td>\n",
       "      <td>-0.055531</td>\n",
       "      <td>-0.055543</td>\n",
       "      <td>-0.019970</td>\n",
       "      <td>-0.039039</td>\n",
       "      <td>-0.015700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186221</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>0.215825</td>\n",
       "      <td>-0.008598</td>\n",
       "      <td>-0.039557</td>\n",
       "      <td>-0.050902</td>\n",
       "      <td>-0.117806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_40</th>\n",
       "      <td>-0.035804</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>-0.067729</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.056930</td>\n",
       "      <td>-0.035725</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>-0.016160</td>\n",
       "      <td>-0.023872</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006616</td>\n",
       "      <td>-0.033399</td>\n",
       "      <td>-0.013628</td>\n",
       "      <td>-0.025390</td>\n",
       "      <td>-0.023944</td>\n",
       "      <td>0.248420</td>\n",
       "      <td>-0.005451</td>\n",
       "      <td>-0.013120</td>\n",
       "      <td>-0.031912</td>\n",
       "      <td>-0.084817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_41</th>\n",
       "      <td>0.012663</td>\n",
       "      <td>-0.032170</td>\n",
       "      <td>-0.028473</td>\n",
       "      <td>-0.005602</td>\n",
       "      <td>-0.070087</td>\n",
       "      <td>-0.031086</td>\n",
       "      <td>-0.037787</td>\n",
       "      <td>-0.030097</td>\n",
       "      <td>-0.039776</td>\n",
       "      <td>-0.030649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.359515</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>0.020480</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-0.039862</td>\n",
       "      <td>-0.026039</td>\n",
       "      <td>-0.114895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_42</th>\n",
       "      <td>-0.017709</td>\n",
       "      <td>-0.051599</td>\n",
       "      <td>0.028387</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>0.274051</td>\n",
       "      <td>-0.059541</td>\n",
       "      <td>-0.057556</td>\n",
       "      <td>-0.035189</td>\n",
       "      <td>-0.041476</td>\n",
       "      <td>-0.050505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>-0.013277</td>\n",
       "      <td>-0.010124</td>\n",
       "      <td>-0.014384</td>\n",
       "      <td>-0.014701</td>\n",
       "      <td>-0.042426</td>\n",
       "      <td>-0.047580</td>\n",
       "      <td>-0.132098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_43</th>\n",
       "      <td>-0.050591</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>-0.044707</td>\n",
       "      <td>-0.007544</td>\n",
       "      <td>-0.053524</td>\n",
       "      <td>-0.046127</td>\n",
       "      <td>-0.061891</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>-0.040021</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082966</td>\n",
       "      <td>0.040190</td>\n",
       "      <td>-0.014289</td>\n",
       "      <td>-0.022285</td>\n",
       "      <td>-0.018053</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>-0.015809</td>\n",
       "      <td>-0.033723</td>\n",
       "      <td>-0.047066</td>\n",
       "      <td>-0.142287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_44</th>\n",
       "      <td>-0.027766</td>\n",
       "      <td>-0.034725</td>\n",
       "      <td>-0.055508</td>\n",
       "      <td>-0.004418</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>-0.038060</td>\n",
       "      <td>-0.038707</td>\n",
       "      <td>-0.024061</td>\n",
       "      <td>-0.032636</td>\n",
       "      <td>-0.023870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013160</td>\n",
       "      <td>-0.020047</td>\n",
       "      <td>-0.008387</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>-0.011302</td>\n",
       "      <td>0.010893</td>\n",
       "      <td>-0.009886</td>\n",
       "      <td>-0.037355</td>\n",
       "      <td>-0.041717</td>\n",
       "      <td>-0.085134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <td>-0.036678</td>\n",
       "      <td>-0.027928</td>\n",
       "      <td>-0.085817</td>\n",
       "      <td>-0.012720</td>\n",
       "      <td>-0.057029</td>\n",
       "      <td>-0.082128</td>\n",
       "      <td>-0.066613</td>\n",
       "      <td>0.056538</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>-0.044976</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091273</td>\n",
       "      <td>-0.023755</td>\n",
       "      <td>0.042780</td>\n",
       "      <td>-0.043761</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>-0.023665</td>\n",
       "      <td>-0.085125</td>\n",
       "      <td>-0.109025</td>\n",
       "      <td>-0.162260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <td>0.005400</td>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.075904</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>-0.059036</td>\n",
       "      <td>-0.068576</td>\n",
       "      <td>-0.042613</td>\n",
       "      <td>-0.052799</td>\n",
       "      <td>-0.043607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>-0.022908</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.033950</td>\n",
       "      <td>-0.015143</td>\n",
       "      <td>-0.054722</td>\n",
       "      <td>-0.056391</td>\n",
       "      <td>-0.158339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <td>-0.019728</td>\n",
       "      <td>-0.006659</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>-0.002874</td>\n",
       "      <td>-0.027709</td>\n",
       "      <td>-0.024670</td>\n",
       "      <td>-0.025366</td>\n",
       "      <td>-0.015039</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>-0.020726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023755</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005304</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>-0.005480</td>\n",
       "      <td>-0.015619</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>-0.050780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <td>-0.004575</td>\n",
       "      <td>-0.042215</td>\n",
       "      <td>-0.031101</td>\n",
       "      <td>-0.005553</td>\n",
       "      <td>-0.054150</td>\n",
       "      <td>-0.045313</td>\n",
       "      <td>-0.044060</td>\n",
       "      <td>-0.011828</td>\n",
       "      <td>-0.042524</td>\n",
       "      <td>-0.019523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042780</td>\n",
       "      <td>-0.022908</td>\n",
       "      <td>-0.005304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011834</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>-0.011883</td>\n",
       "      <td>-0.034752</td>\n",
       "      <td>-0.018871</td>\n",
       "      <td>-0.108749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <td>-0.038761</td>\n",
       "      <td>0.015804</td>\n",
       "      <td>-0.045449</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.042927</td>\n",
       "      <td>-0.016615</td>\n",
       "      <td>-0.036877</td>\n",
       "      <td>-0.028695</td>\n",
       "      <td>-0.028991</td>\n",
       "      <td>-0.007730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043761</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.011834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>0.063060</td>\n",
       "      <td>-0.069016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <td>-0.013797</td>\n",
       "      <td>-0.058462</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>-0.082370</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.073792</td>\n",
       "      <td>-0.062708</td>\n",
       "      <td>-0.055444</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.033950</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>-0.014177</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023268</td>\n",
       "      <td>-0.062093</td>\n",
       "      <td>-0.030197</td>\n",
       "      <td>-0.148933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <td>0.008485</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>0.032936</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>-0.010733</td>\n",
       "      <td>0.056789</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.065809</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023665</td>\n",
       "      <td>-0.015143</td>\n",
       "      <td>-0.005480</td>\n",
       "      <td>-0.011883</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.023268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602551</td>\n",
       "      <td>0.119532</td>\n",
       "      <td>0.090763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <td>0.067895</td>\n",
       "      <td>0.148034</td>\n",
       "      <td>0.091458</td>\n",
       "      <td>0.046516</td>\n",
       "      <td>0.038279</td>\n",
       "      <td>0.126238</td>\n",
       "      <td>0.087923</td>\n",
       "      <td>0.072371</td>\n",
       "      <td>0.248130</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085125</td>\n",
       "      <td>-0.054722</td>\n",
       "      <td>-0.015619</td>\n",
       "      <td>-0.034752</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>-0.062093</td>\n",
       "      <td>0.602551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482246</td>\n",
       "      <td>0.301983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StylizedLetters</th>\n",
       "      <td>0.053014</td>\n",
       "      <td>0.047688</td>\n",
       "      <td>0.024856</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>0.051525</td>\n",
       "      <td>-0.032189</td>\n",
       "      <td>0.034428</td>\n",
       "      <td>0.194155</td>\n",
       "      <td>0.054715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109025</td>\n",
       "      <td>-0.056391</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>-0.018871</td>\n",
       "      <td>0.063060</td>\n",
       "      <td>-0.030197</td>\n",
       "      <td>0.119532</td>\n",
       "      <td>0.482246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsGoodNews</th>\n",
       "      <td>0.118213</td>\n",
       "      <td>0.170886</td>\n",
       "      <td>0.167442</td>\n",
       "      <td>0.048563</td>\n",
       "      <td>0.231699</td>\n",
       "      <td>0.259183</td>\n",
       "      <td>0.394999</td>\n",
       "      <td>0.181393</td>\n",
       "      <td>0.226195</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162260</td>\n",
       "      <td>-0.158339</td>\n",
       "      <td>-0.050780</td>\n",
       "      <td>-0.108749</td>\n",
       "      <td>-0.069016</td>\n",
       "      <td>-0.148933</td>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.301983</td>\n",
       "      <td>0.258844</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  \\\n",
       "Freq_Of_Word_1                1.000000        0.010394       -0.002367   \n",
       "Freq_Of_Word_2                0.010394        1.000000        0.080100   \n",
       "Freq_Of_Word_3               -0.002367        0.080100        1.000000   \n",
       "Freq_Of_Word_4               -0.013153       -0.012772       -0.021998   \n",
       "Freq_Of_Word_5                0.020966        0.004583        0.130316   \n",
       "Freq_Of_Word_6                0.081520       -0.003339        0.077990   \n",
       "Freq_Of_Word_7                0.047176        0.172478       -0.011462   \n",
       "Freq_Of_Word_8               -0.008335        0.001162        0.010934   \n",
       "Freq_Of_Word_9                0.096471        0.043161        0.062003   \n",
       "Freq_Of_Word_10               0.048763        0.210548        0.023857   \n",
       "Freq_Of_Word_11               0.045993        0.154969        0.054761   \n",
       "Freq_Of_Word_12               0.104085        0.051260        0.054657   \n",
       "Freq_Of_Word_13               0.064742        0.034448        0.054648   \n",
       "Freq_Of_Word_14               0.010435        0.014103        0.035432   \n",
       "Freq_Of_Word_15               0.019888        0.106974        0.151548   \n",
       "Freq_Of_Word_16               0.061790        0.117832        0.094623   \n",
       "Freq_Of_Word_17               0.053233        0.021727        0.007095   \n",
       "Freq_Of_Word_18               0.002784        0.240146        0.105804   \n",
       "Freq_Of_Word_19               0.135201        0.127997        0.117618   \n",
       "Freq_Of_Word_20               0.024064       -0.013738        0.023329   \n",
       "Freq_Of_Word_21               0.073889        0.178618        0.095158   \n",
       "Freq_Of_Word_22              -0.033844        0.006795       -0.044568   \n",
       "Freq_Of_Word_23               0.188110        0.038223        0.161322   \n",
       "Freq_Of_Word_24               0.233784        0.060765        0.089256   \n",
       "Freq_Of_Word_25              -0.066760       -0.045073       -0.090321   \n",
       "Freq_Of_Word_26              -0.051950       -0.026722       -0.070649   \n",
       "Freq_Of_Word_27              -0.065578       -0.038330       -0.072454   \n",
       "Freq_Of_Word_28              -0.059961       -0.045148       -0.039450   \n",
       "Freq_Of_Word_29              -0.044040       -0.038957       -0.040833   \n",
       "Freq_Of_Word_30              -0.052091       -0.029210       -0.023884   \n",
       "Freq_Of_Word_31              -0.043987       -0.026008       -0.050324   \n",
       "Freq_Of_Word_32              -0.031670       -0.012993       -0.058553   \n",
       "Freq_Of_Word_33              -0.046447       -0.044344       -0.073300   \n",
       "Freq_Of_Word_34              -0.031645       -0.017524       -0.061238   \n",
       "Freq_Of_Word_35              -0.055118       -0.029811       -0.061394   \n",
       "Freq_Of_Word_36              -0.047682       -0.043307       -0.044662   \n",
       "Freq_Of_Word_37              -0.066333        0.015977       -0.076767   \n",
       "Freq_Of_Word_38              -0.018651       -0.014217        0.143998   \n",
       "Freq_Of_Word_39              -0.014406       -0.017042       -0.037474   \n",
       "Freq_Of_Word_40              -0.035804        0.022986       -0.067729   \n",
       "Freq_Of_Word_41               0.012663       -0.032170       -0.028473   \n",
       "Freq_Of_Word_42              -0.017709       -0.051599        0.028387   \n",
       "Freq_Of_Word_43              -0.050591        0.070833       -0.044707   \n",
       "Freq_Of_Word_44              -0.027766       -0.034725       -0.055508   \n",
       "Freq_Of_Word_45              -0.036678       -0.027928       -0.085817   \n",
       "Freq_Of_Word_46               0.005400       -0.017376       -0.075904   \n",
       "Freq_Of_Word_47              -0.019728       -0.006659       -0.017425   \n",
       "Freq_Of_Word_48              -0.004575       -0.042215       -0.031101   \n",
       "Freq_Of_Word_49              -0.038761        0.015804       -0.045449   \n",
       "Freq_Of_Word_50              -0.013797       -0.058462        0.000186   \n",
       "TotalEmojiCharacters          0.008485        0.092289        0.032936   \n",
       "LengthOFFirstParagraph        0.067895        0.148034        0.091458   \n",
       "StylizedLetters               0.053014        0.047688        0.024856   \n",
       "IsGoodNews                    0.118213        0.170886        0.167442   \n",
       "\n",
       "                        Freq_Of_Word_4  Freq_Of_Word_5  Freq_Of_Word_6  \\\n",
       "Freq_Of_Word_1               -0.013153        0.020966        0.081520   \n",
       "Freq_Of_Word_2               -0.012772        0.004583       -0.003339   \n",
       "Freq_Of_Word_3               -0.021998        0.130316        0.077990   \n",
       "Freq_Of_Word_4                1.000000       -0.016472       -0.014258   \n",
       "Freq_Of_Word_5               -0.016472        1.000000        0.064095   \n",
       "Freq_Of_Word_6               -0.014258        0.064095        1.000000   \n",
       "Freq_Of_Word_7                0.032053        0.100868        0.089194   \n",
       "Freq_Of_Word_8               -0.004728        0.014420        0.064522   \n",
       "Freq_Of_Word_9               -0.005920        0.018507        0.081660   \n",
       "Freq_Of_Word_10               0.006542        0.010941        0.008806   \n",
       "Freq_Of_Word_11              -0.012150        0.078550        0.020385   \n",
       "Freq_Of_Word_12              -0.007582        0.094738        0.010033   \n",
       "Freq_Of_Word_13              -0.014091        0.031600        0.149552   \n",
       "Freq_Of_Word_14              -0.007445        0.008090        0.089109   \n",
       "Freq_Of_Word_15               0.054825        0.050210        0.182595   \n",
       "Freq_Of_Word_16              -0.014719        0.080174        0.053130   \n",
       "Freq_Of_Word_17               0.002994        0.112351        0.053337   \n",
       "Freq_Of_Word_18               0.062028        0.015380        0.044340   \n",
       "Freq_Of_Word_19               0.018304        0.094023        0.083722   \n",
       "Freq_Of_Word_20              -0.003336        0.033087        0.053960   \n",
       "Freq_Of_Word_21               0.036530        0.144796        0.186967   \n",
       "Freq_Of_Word_22              -0.005475       -0.040629        0.006604   \n",
       "Freq_Of_Word_23               0.114027        0.089794        0.266449   \n",
       "Freq_Of_Word_24              -0.012118        0.068485        0.142167   \n",
       "Freq_Of_Word_25              -0.012576       -0.104150       -0.094234   \n",
       "Freq_Of_Word_26              -0.011417       -0.094840       -0.096180   \n",
       "Freq_Of_Word_27              -0.008820       -0.072034       -0.039960   \n",
       "Freq_Of_Word_28              -0.008664       -0.078768       -0.071675   \n",
       "Freq_Of_Word_29              -0.006954        0.170280       -0.051692   \n",
       "Freq_Of_Word_30              -0.009199       -0.086614       -0.055782   \n",
       "Freq_Of_Word_31              -0.006765       -0.066002       -0.047805   \n",
       "Freq_Of_Word_32              -0.006173       -0.052345       -0.027583   \n",
       "Freq_Of_Word_33              -0.006415       -0.049696       -0.051187   \n",
       "Freq_Of_Word_34              -0.006173       -0.051212       -0.027228   \n",
       "Freq_Of_Word_35              -0.008994       -0.075444       -0.068704   \n",
       "Freq_Of_Word_36              -0.010228       -0.080465       -0.058658   \n",
       "Freq_Of_Word_37               0.018527       -0.091039       -0.100688   \n",
       "Freq_Of_Word_38              -0.002481        0.128054       -0.022917   \n",
       "Freq_Of_Word_39              -0.006443       -0.044715       -0.055531   \n",
       "Freq_Of_Word_40              -0.007435       -0.056930       -0.035725   \n",
       "Freq_Of_Word_41              -0.005602       -0.070087       -0.031086   \n",
       "Freq_Of_Word_42              -0.006524        0.274051       -0.059541   \n",
       "Freq_Of_Word_43              -0.007544       -0.053524       -0.046127   \n",
       "Freq_Of_Word_44              -0.004418       -0.021884       -0.038060   \n",
       "Freq_Of_Word_45              -0.012720       -0.057029       -0.082128   \n",
       "Freq_Of_Word_46              -0.008082       -0.086024       -0.059036   \n",
       "Freq_Of_Word_47              -0.002874       -0.027709       -0.024670   \n",
       "Freq_Of_Word_48              -0.005553       -0.054150       -0.045313   \n",
       "Freq_Of_Word_49               0.001837       -0.042927       -0.016615   \n",
       "Freq_Of_Word_50              -0.020997       -0.082370        0.015599   \n",
       "TotalEmojiCharacters          0.004771        0.003146       -0.010733   \n",
       "LengthOFFirstParagraph        0.046516        0.038279        0.126238   \n",
       "StylizedLetters               0.013794       -0.021524        0.051525   \n",
       "IsGoodNews                    0.048563        0.231699        0.259183   \n",
       "\n",
       "                        Freq_Of_Word_7  Freq_Of_Word_8  Freq_Of_Word_9  \\\n",
       "Freq_Of_Word_1                0.047176       -0.008335        0.096471   \n",
       "Freq_Of_Word_2                0.172478        0.001162        0.043161   \n",
       "Freq_Of_Word_3               -0.011462        0.010934        0.062003   \n",
       "Freq_Of_Word_4                0.032053       -0.004728       -0.005920   \n",
       "Freq_Of_Word_5                0.100868        0.014420        0.018507   \n",
       "Freq_Of_Word_6                0.089194        0.064522        0.081660   \n",
       "Freq_Of_Word_7                1.000000        0.043794        0.106784   \n",
       "Freq_Of_Word_8                0.043794        1.000000        0.110751   \n",
       "Freq_Of_Word_9                0.106784        0.110751        1.000000   \n",
       "Freq_Of_Word_10               0.083965        0.040782        0.107459   \n",
       "Freq_Of_Word_11               0.337929        0.152854        0.144659   \n",
       "Freq_Of_Word_12              -0.018704       -0.016754        0.025835   \n",
       "Freq_Of_Word_13              -0.030903        0.007588        0.064029   \n",
       "Freq_Of_Word_14              -0.057257        0.002460        0.047216   \n",
       "Freq_Of_Word_15               0.042979        0.068678        0.241813   \n",
       "Freq_Of_Word_16               0.194472        0.145406        0.001198   \n",
       "Freq_Of_Word_17               0.172559        0.232003        0.284616   \n",
       "Freq_Of_Word_18               0.138829        0.016120        0.066808   \n",
       "Freq_Of_Word_19               0.116231       -0.010299        0.037624   \n",
       "Freq_Of_Word_20               0.084406        0.187753        0.195269   \n",
       "Freq_Of_Word_21               0.198694        0.313332        0.181550   \n",
       "Freq_Of_Word_22              -0.028517       -0.025514       -0.021318   \n",
       "Freq_Of_Word_23               0.123591        0.047161        0.164176   \n",
       "Freq_Of_Word_24               0.092412        0.069177        0.193540   \n",
       "Freq_Of_Word_25              -0.098229       -0.049856       -0.077909   \n",
       "Freq_Of_Word_26              -0.085849       -0.047542       -0.063494   \n",
       "Freq_Of_Word_27              -0.076389       -0.047792       -0.065843   \n",
       "Freq_Of_Word_28              -0.075435       -0.033622       -0.050080   \n",
       "Freq_Of_Word_29              -0.061369       -0.033022       -0.049772   \n",
       "Freq_Of_Word_30              -0.071875       -0.035481       -0.054701   \n",
       "Freq_Of_Word_31              -0.059636       -0.035443       -0.044865   \n",
       "Freq_Of_Word_32              -0.054480       -0.032307       -0.040725   \n",
       "Freq_Of_Word_33              -0.037426       -0.033709       -0.028231   \n",
       "Freq_Of_Word_34              -0.054472       -0.032300       -0.040712   \n",
       "Freq_Of_Word_35              -0.068418       -0.031703       -0.037520   \n",
       "Freq_Of_Word_36              -0.068495       -0.037430       -0.051631   \n",
       "Freq_Of_Word_37              -0.035034       -0.018174       -0.035020   \n",
       "Freq_Of_Word_38              -0.014861       -0.004863       -0.004502   \n",
       "Freq_Of_Word_39              -0.055543       -0.019970       -0.039039   \n",
       "Freq_Of_Word_40              -0.033984       -0.016160       -0.023872   \n",
       "Freq_Of_Word_41              -0.037787       -0.030097       -0.039776   \n",
       "Freq_Of_Word_42              -0.057556       -0.035189       -0.041476   \n",
       "Freq_Of_Word_43              -0.061891        0.019288       -0.040021   \n",
       "Freq_Of_Word_44              -0.038707       -0.024061       -0.032636   \n",
       "Freq_Of_Word_45              -0.066613        0.056538       -0.078517   \n",
       "Freq_Of_Word_46              -0.068576       -0.042613       -0.052799   \n",
       "Freq_Of_Word_47              -0.025366       -0.015039       -0.002607   \n",
       "Freq_Of_Word_48              -0.044060       -0.011828       -0.042524   \n",
       "Freq_Of_Word_49              -0.036877       -0.028695       -0.028991   \n",
       "Freq_Of_Word_50              -0.073792       -0.062708       -0.055444   \n",
       "TotalEmojiCharacters          0.056789        0.018629        0.065809   \n",
       "LengthOFFirstParagraph        0.087923        0.072371        0.248130   \n",
       "StylizedLetters              -0.032189        0.034428        0.194155   \n",
       "IsGoodNews                    0.394999        0.181393        0.226195   \n",
       "\n",
       "                        Freq_Of_Word_10  ...  Freq_Of_Word_45  \\\n",
       "Freq_Of_Word_1                 0.048763  ...        -0.036678   \n",
       "Freq_Of_Word_2                 0.210548  ...        -0.027928   \n",
       "Freq_Of_Word_3                 0.023857  ...        -0.085817   \n",
       "Freq_Of_Word_4                 0.006542  ...        -0.012720   \n",
       "Freq_Of_Word_5                 0.010941  ...        -0.057029   \n",
       "Freq_Of_Word_6                 0.008806  ...        -0.082128   \n",
       "Freq_Of_Word_7                 0.083965  ...        -0.066613   \n",
       "Freq_Of_Word_8                 0.040782  ...         0.056538   \n",
       "Freq_Of_Word_9                 0.107459  ...        -0.078517   \n",
       "Freq_Of_Word_10                1.000000  ...        -0.044976   \n",
       "Freq_Of_Word_11                0.143873  ...        -0.090731   \n",
       "Freq_Of_Word_12                0.082312  ...        -0.102704   \n",
       "Freq_Of_Word_13                0.005756  ...        -0.083497   \n",
       "Freq_Of_Word_14                0.010584  ...         0.015234   \n",
       "Freq_Of_Word_15                0.126007  ...        -0.037353   \n",
       "Freq_Of_Word_16                0.023513  ...        -0.079356   \n",
       "Freq_Of_Word_17                0.105277  ...        -0.075456   \n",
       "Freq_Of_Word_18                0.018174  ...        -0.066540   \n",
       "Freq_Of_Word_19                0.035888  ...         0.084670   \n",
       "Freq_Of_Word_20                0.045806  ...        -0.054071   \n",
       "Freq_Of_Word_21                0.065709  ...        -0.008939   \n",
       "Freq_Of_Word_22               -0.009059  ...        -0.041964   \n",
       "Freq_Of_Word_23                0.073070  ...        -0.064023   \n",
       "Freq_Of_Word_24                0.082154  ...        -0.072353   \n",
       "Freq_Of_Word_25               -0.046410  ...         0.099259   \n",
       "Freq_Of_Word_26               -0.014577  ...        -0.032278   \n",
       "Freq_Of_Word_27               -0.041037  ...         0.075348   \n",
       "Freq_Of_Word_28                0.022498  ...        -0.022780   \n",
       "Freq_Of_Word_29               -0.023312  ...        -0.005481   \n",
       "Freq_Of_Word_30               -0.003592  ...        -0.021652   \n",
       "Freq_Of_Word_31               -0.006927  ...         0.003870   \n",
       "Freq_Of_Word_32               -0.006383  ...         0.009678   \n",
       "Freq_Of_Word_33               -0.044143  ...         0.035136   \n",
       "Freq_Of_Word_34               -0.006342  ...         0.009733   \n",
       "Freq_Of_Word_35                0.007908  ...        -0.022133   \n",
       "Freq_Of_Word_36               -0.000463  ...        -0.014576   \n",
       "Freq_Of_Word_37               -0.015782  ...        -0.005131   \n",
       "Freq_Of_Word_38               -0.016683  ...         0.009930   \n",
       "Freq_Of_Word_39               -0.015700  ...         0.186221   \n",
       "Freq_Of_Word_40                0.009640  ...        -0.006616   \n",
       "Freq_Of_Word_41               -0.030649  ...         0.023265   \n",
       "Freq_Of_Word_42               -0.050505  ...         0.023710   \n",
       "Freq_Of_Word_43                0.019185  ...         0.082966   \n",
       "Freq_Of_Word_44               -0.023870  ...        -0.013160   \n",
       "Freq_Of_Word_45               -0.044976  ...         1.000000   \n",
       "Freq_Of_Word_46               -0.043607  ...         0.091273   \n",
       "Freq_Of_Word_47               -0.020726  ...        -0.023755   \n",
       "Freq_Of_Word_48               -0.019523  ...         0.042780   \n",
       "Freq_Of_Word_49               -0.007730  ...        -0.043761   \n",
       "Freq_Of_Word_50               -0.023969  ...         0.014797   \n",
       "TotalEmojiCharacters           0.043950  ...        -0.023665   \n",
       "LengthOFFirstParagraph         0.142119  ...        -0.085125   \n",
       "StylizedLetters                0.054715  ...        -0.109025   \n",
       "IsGoodNews                     0.092510  ...        -0.162260   \n",
       "\n",
       "                        Freq_Of_Word_46  Freq_Of_Word_47  Freq_Of_Word_48  \\\n",
       "Freq_Of_Word_1                 0.005400        -0.019728        -0.004575   \n",
       "Freq_Of_Word_2                -0.017376        -0.006659        -0.042215   \n",
       "Freq_Of_Word_3                -0.075904        -0.017425        -0.031101   \n",
       "Freq_Of_Word_4                -0.008082        -0.002874        -0.005553   \n",
       "Freq_Of_Word_5                -0.086024        -0.027709        -0.054150   \n",
       "Freq_Of_Word_6                -0.059036        -0.024670        -0.045313   \n",
       "Freq_Of_Word_7                -0.068576        -0.025366        -0.044060   \n",
       "Freq_Of_Word_8                -0.042613        -0.015039        -0.011828   \n",
       "Freq_Of_Word_9                -0.052799        -0.002607        -0.042524   \n",
       "Freq_Of_Word_10               -0.043607        -0.020726        -0.019523   \n",
       "Freq_Of_Word_11               -0.058824        -0.017508        -0.033603   \n",
       "Freq_Of_Word_12               -0.070267         0.108387         0.044370   \n",
       "Freq_Of_Word_13               -0.014755         0.009513        -0.014958   \n",
       "Freq_Of_Word_14               -0.025687        -0.009127        -0.026366   \n",
       "Freq_Of_Word_15               -0.026902        -0.011531        -0.026295   \n",
       "Freq_Of_Word_16               -0.068662        -0.027590        -0.036098   \n",
       "Freq_Of_Word_17               -0.064052         0.006148        -0.035204   \n",
       "Freq_Of_Word_18               -0.045892        -0.025144        -0.026060   \n",
       "Freq_Of_Word_19               -0.004198        -0.055564         0.041264   \n",
       "Freq_Of_Word_20               -0.042216        -0.004252        -0.029482   \n",
       "Freq_Of_Word_21               -0.094646        -0.046155        -0.067190   \n",
       "Freq_Of_Word_22               -0.029263        -0.010413        -0.020075   \n",
       "Freq_Of_Word_23               -0.046672        -0.016891        -0.035599   \n",
       "Freq_Of_Word_24               -0.048612        -0.010971        -0.045024   \n",
       "Freq_Of_Word_25               -0.039051        -0.011618        -0.000764   \n",
       "Freq_Of_Word_26               -0.019293         0.023479         0.007272   \n",
       "Freq_Of_Word_27               -0.036627        -0.016047         0.045517   \n",
       "Freq_Of_Word_28               -0.041392        -0.016480         0.041606   \n",
       "Freq_Of_Word_29               -0.033545        -0.013212         0.002944   \n",
       "Freq_Of_Word_30               -0.033445        -0.017496        -0.011558   \n",
       "Freq_Of_Word_31               -0.029767        -0.012868        -0.023175   \n",
       "Freq_Of_Word_32               -0.026552        -0.011742        -0.022683   \n",
       "Freq_Of_Word_33               -0.008280         0.036903         0.026818   \n",
       "Freq_Of_Word_34               -0.026542        -0.011740        -0.022679   \n",
       "Freq_Of_Word_35               -0.041579         0.014363         0.019339   \n",
       "Freq_Of_Word_36               -0.047337         0.048219        -0.001036   \n",
       "Freq_Of_Word_37                0.176820         0.003069         0.050839   \n",
       "Freq_Of_Word_38               -0.005298        -0.004682        -0.001800   \n",
       "Freq_Of_Word_39               -0.016704        -0.008847         0.002224   \n",
       "Freq_Of_Word_40               -0.033399        -0.013628        -0.025390   \n",
       "Freq_Of_Word_41                0.359515         0.006385        -0.017691   \n",
       "Freq_Of_Word_42               -0.029872        -0.007970        -0.013277   \n",
       "Freq_Of_Word_43                0.040190        -0.014289        -0.022285   \n",
       "Freq_Of_Word_44               -0.020047        -0.008387         0.007490   \n",
       "Freq_Of_Word_45                0.091273        -0.023755         0.042780   \n",
       "Freq_Of_Word_46                1.000000         0.009462        -0.022908   \n",
       "Freq_Of_Word_47                0.009462         1.000000        -0.005304   \n",
       "Freq_Of_Word_48               -0.022908        -0.005304         1.000000   \n",
       "Freq_Of_Word_49               -0.020983         0.012545        -0.011834   \n",
       "Freq_Of_Word_50                0.033950         0.005599        -0.014177   \n",
       "TotalEmojiCharacters          -0.015143        -0.005480        -0.011883   \n",
       "LengthOFFirstParagraph        -0.054722        -0.015619        -0.034752   \n",
       "StylizedLetters               -0.056391         0.004292        -0.018871   \n",
       "IsGoodNews                    -0.158339        -0.050780        -0.108749   \n",
       "\n",
       "                        Freq_Of_Word_49  Freq_Of_Word_50  \\\n",
       "Freq_Of_Word_1                -0.038761        -0.013797   \n",
       "Freq_Of_Word_2                 0.015804        -0.058462   \n",
       "Freq_Of_Word_3                -0.045449         0.000186   \n",
       "Freq_Of_Word_4                 0.001837        -0.020997   \n",
       "Freq_Of_Word_5                -0.042927        -0.082370   \n",
       "Freq_Of_Word_6                -0.016615         0.015599   \n",
       "Freq_Of_Word_7                -0.036877        -0.073792   \n",
       "Freq_Of_Word_8                -0.028695        -0.062708   \n",
       "Freq_Of_Word_9                -0.028991        -0.055444   \n",
       "Freq_Of_Word_10               -0.007730        -0.023969   \n",
       "Freq_Of_Word_11               -0.038619        -0.088521   \n",
       "Freq_Of_Word_12               -0.016121        -0.032481   \n",
       "Freq_Of_Word_13               -0.039947        -0.089409   \n",
       "Freq_Of_Word_14               -0.023472         0.008222   \n",
       "Freq_Of_Word_15               -0.025377        -0.047241   \n",
       "Freq_Of_Word_16               -0.049883        -0.073027   \n",
       "Freq_Of_Word_17               -0.022935        -0.068534   \n",
       "Freq_Of_Word_18               -0.043155        -0.060639   \n",
       "Freq_Of_Word_19               -0.049389        -0.167888   \n",
       "Freq_Of_Word_20               -0.021745        -0.055709   \n",
       "Freq_Of_Word_21               -0.061972        -0.111004   \n",
       "Freq_Of_Word_22                0.513042        -0.057050   \n",
       "Freq_Of_Word_23               -0.017824        -0.056141   \n",
       "Freq_Of_Word_24               -0.034123        -0.068233   \n",
       "Freq_Of_Word_25                0.057537         0.150820   \n",
       "Freq_Of_Word_26                0.004102         0.109468   \n",
       "Freq_Of_Word_27               -0.023548         0.052358   \n",
       "Freq_Of_Word_28               -0.029745         0.326506   \n",
       "Freq_Of_Word_29               -0.024418         0.148778   \n",
       "Freq_Of_Word_30               -0.017259         0.208994   \n",
       "Freq_Of_Word_31               -0.020359         0.268670   \n",
       "Freq_Of_Word_32               -0.013075         0.285373   \n",
       "Freq_Of_Word_33               -0.011216         0.055633   \n",
       "Freq_Of_Word_34               -0.013402         0.285487   \n",
       "Freq_Of_Word_35               -0.029379         0.202351   \n",
       "Freq_Of_Word_36               -0.030069         0.236874   \n",
       "Freq_Of_Word_37                0.023104         0.126325   \n",
       "Freq_Of_Word_38               -0.008184        -0.002331   \n",
       "Freq_Of_Word_39                0.043864         0.215825   \n",
       "Freq_Of_Word_40               -0.023944         0.248420   \n",
       "Freq_Of_Word_41                0.011809         0.020480   \n",
       "Freq_Of_Word_42               -0.010124        -0.014384   \n",
       "Freq_Of_Word_43               -0.018053         0.037643   \n",
       "Freq_Of_Word_44               -0.011302         0.010893   \n",
       "Freq_Of_Word_45               -0.043761         0.014797   \n",
       "Freq_Of_Word_46               -0.020983         0.033950   \n",
       "Freq_Of_Word_47                0.012545         0.005599   \n",
       "Freq_Of_Word_48               -0.011834        -0.014177   \n",
       "Freq_Of_Word_49                1.000000         0.029329   \n",
       "Freq_Of_Word_50                0.029329         1.000000   \n",
       "TotalEmojiCharacters           0.001313        -0.023268   \n",
       "LengthOFFirstParagraph         0.020939        -0.062093   \n",
       "StylizedLetters                0.063060        -0.030197   \n",
       "IsGoodNews                    -0.069016        -0.148933   \n",
       "\n",
       "                        TotalEmojiCharacters  LengthOFFirstParagraph  \\\n",
       "Freq_Of_Word_1                      0.008485                0.067895   \n",
       "Freq_Of_Word_2                      0.092289                0.148034   \n",
       "Freq_Of_Word_3                      0.032936                0.091458   \n",
       "Freq_Of_Word_4                      0.004771                0.046516   \n",
       "Freq_Of_Word_5                      0.003146                0.038279   \n",
       "Freq_Of_Word_6                     -0.010733                0.126238   \n",
       "Freq_Of_Word_7                      0.056789                0.087923   \n",
       "Freq_Of_Word_8                      0.018629                0.072371   \n",
       "Freq_Of_Word_9                      0.065809                0.248130   \n",
       "Freq_Of_Word_10                     0.043950                0.142119   \n",
       "Freq_Of_Word_11                     0.057045                0.147225   \n",
       "Freq_Of_Word_12                    -0.000067                0.033398   \n",
       "Freq_Of_Word_13                    -0.016536                0.070099   \n",
       "Freq_Of_Word_14                     0.000861                0.061695   \n",
       "Freq_Of_Word_15                     0.033021                0.419893   \n",
       "Freq_Of_Word_16                     0.001139                0.026770   \n",
       "Freq_Of_Word_17                     0.053995                0.132124   \n",
       "Freq_Of_Word_18                    -0.010902                0.096371   \n",
       "Freq_Of_Word_19                    -0.030387                0.011840   \n",
       "Freq_Of_Word_20                     0.031836                0.128650   \n",
       "Freq_Of_Word_21                     0.035218                0.119826   \n",
       "Freq_Of_Word_22                     0.015269                0.084280   \n",
       "Freq_Of_Word_23                     0.016502                0.262555   \n",
       "Freq_Of_Word_24                     0.063139                0.227742   \n",
       "Freq_Of_Word_25                    -0.019093               -0.068274   \n",
       "Freq_Of_Word_26                    -0.018088               -0.065806   \n",
       "Freq_Of_Word_27                    -0.018701               -0.073944   \n",
       "Freq_Of_Word_28                    -0.010961               -0.049216   \n",
       "Freq_Of_Word_29                    -0.013070               -0.054105   \n",
       "Freq_Of_Word_30                    -0.014952               -0.058283   \n",
       "Freq_Of_Word_31                    -0.009690               -0.041002   \n",
       "Freq_Of_Word_32                    -0.009745               -0.044899   \n",
       "Freq_Of_Word_33                    -0.012850               -0.036991   \n",
       "Freq_Of_Word_34                    -0.009329               -0.042076   \n",
       "Freq_Of_Word_35                    -0.009562               -0.040038   \n",
       "Freq_Of_Word_36                    -0.017428               -0.057942   \n",
       "Freq_Of_Word_37                    -0.009267               -0.040567   \n",
       "Freq_Of_Word_38                    -0.005105               -0.019169   \n",
       "Freq_Of_Word_39                    -0.008598               -0.039557   \n",
       "Freq_Of_Word_40                    -0.005451               -0.013120   \n",
       "Freq_Of_Word_41                    -0.010269               -0.039862   \n",
       "Freq_Of_Word_42                    -0.014701               -0.042426   \n",
       "Freq_Of_Word_43                    -0.015809               -0.033723   \n",
       "Freq_Of_Word_44                    -0.009886               -0.037355   \n",
       "Freq_Of_Word_45                    -0.023665               -0.085125   \n",
       "Freq_Of_Word_46                    -0.015143               -0.054722   \n",
       "Freq_Of_Word_47                    -0.005480               -0.015619   \n",
       "Freq_Of_Word_48                    -0.011883               -0.034752   \n",
       "Freq_Of_Word_49                     0.001313                0.020939   \n",
       "Freq_Of_Word_50                    -0.023268               -0.062093   \n",
       "TotalEmojiCharacters                1.000000                0.602551   \n",
       "LengthOFFirstParagraph              0.602551                1.000000   \n",
       "StylizedLetters                     0.119532                0.482246   \n",
       "IsGoodNews                          0.090763                0.301983   \n",
       "\n",
       "                        StylizedLetters  IsGoodNews  \n",
       "Freq_Of_Word_1                 0.053014    0.118213  \n",
       "Freq_Of_Word_2                 0.047688    0.170886  \n",
       "Freq_Of_Word_3                 0.024856    0.167442  \n",
       "Freq_Of_Word_4                 0.013794    0.048563  \n",
       "Freq_Of_Word_5                -0.021524    0.231699  \n",
       "Freq_Of_Word_6                 0.051525    0.259183  \n",
       "Freq_Of_Word_7                -0.032189    0.394999  \n",
       "Freq_Of_Word_8                 0.034428    0.181393  \n",
       "Freq_Of_Word_9                 0.194155    0.226195  \n",
       "Freq_Of_Word_10                0.054715    0.092510  \n",
       "Freq_Of_Word_11                0.097364    0.305149  \n",
       "Freq_Of_Word_12               -0.016894   -0.008034  \n",
       "Freq_Of_Word_13                0.098717    0.111134  \n",
       "Freq_Of_Word_14                0.120967   -0.002024  \n",
       "Freq_Of_Word_15                0.175635    0.194164  \n",
       "Freq_Of_Word_16               -0.015521    0.346826  \n",
       "Freq_Of_Word_17                0.042107    0.234857  \n",
       "Freq_Of_Word_18                0.027136    0.182224  \n",
       "Freq_Of_Word_19               -0.033011    0.235022  \n",
       "Freq_Of_Word_20                0.075401    0.244805  \n",
       "Freq_Of_Word_21                0.026482    0.403225  \n",
       "Freq_Of_Word_22                0.149200    0.094523  \n",
       "Freq_Of_Word_23                0.182349    0.325740  \n",
       "Freq_Of_Word_24                0.193163    0.359695  \n",
       "Freq_Of_Word_25               -0.041265   -0.251108  \n",
       "Freq_Of_Word_26               -0.054896   -0.226468  \n",
       "Freq_Of_Word_27               -0.086898   -0.180674  \n",
       "Freq_Of_Word_28               -0.064887   -0.129274  \n",
       "Freq_Of_Word_29               -0.059172   -0.142238  \n",
       "Freq_Of_Word_30               -0.065686   -0.187425  \n",
       "Freq_Of_Word_31               -0.048894   -0.138745  \n",
       "Freq_Of_Word_32               -0.050110   -0.123148  \n",
       "Freq_Of_Word_33               -0.008380   -0.116501  \n",
       "Freq_Of_Word_34               -0.044117   -0.125556  \n",
       "Freq_Of_Word_35               -0.046255   -0.168817  \n",
       "Freq_Of_Word_36               -0.048603   -0.145174  \n",
       "Freq_Of_Word_37               -0.008354   -0.166855  \n",
       "Freq_Of_Word_38               -0.017020   -0.038689  \n",
       "Freq_Of_Word_39               -0.050902   -0.117806  \n",
       "Freq_Of_Word_40               -0.031912   -0.084817  \n",
       "Freq_Of_Word_41               -0.026039   -0.114895  \n",
       "Freq_Of_Word_42               -0.047580   -0.132098  \n",
       "Freq_Of_Word_43               -0.047066   -0.142287  \n",
       "Freq_Of_Word_44               -0.041717   -0.085134  \n",
       "Freq_Of_Word_45               -0.109025   -0.162260  \n",
       "Freq_Of_Word_46               -0.056391   -0.158339  \n",
       "Freq_Of_Word_47                0.004292   -0.050780  \n",
       "Freq_Of_Word_48               -0.018871   -0.108749  \n",
       "Freq_Of_Word_49                0.063060   -0.069016  \n",
       "Freq_Of_Word_50               -0.030197   -0.148933  \n",
       "TotalEmojiCharacters           0.119532    0.090763  \n",
       "LengthOFFirstParagraph         0.482246    0.301983  \n",
       "StylizedLetters                1.000000    0.258844  \n",
       "IsGoodNews                     0.258844    1.000000  \n",
       "\n",
       "[54 rows x 54 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X_train,y_train,test_size=0.2,random_state=5,stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsGoodNews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     IsGoodNews\n",
       "889           0\n",
       "657           0\n",
       "881           1\n",
       "128           0\n",
       "662           0\n",
       "90            0\n",
       "752           1\n",
       "173           1\n",
       "675           0\n",
       "896           0\n",
       "825           0\n",
       "43            0\n",
       "325           0\n",
       "320           0\n",
       "747           1\n",
       "252           1\n",
       "694           0\n",
       "265           1\n",
       "323           1\n",
       "10            0\n",
       "769           1\n",
       "101           0\n",
       "186           0\n",
       "29            0\n",
       "741           1\n",
       "604           0\n",
       "121           0\n",
       "149           1\n",
       "130           0\n",
       "726           1\n",
       "..          ...\n",
       "62            1\n",
       "309           0\n",
       "343           0\n",
       "862           1\n",
       "364           0\n",
       "593           1\n",
       "602           1\n",
       "690           1\n",
       "788           1\n",
       "178           0\n",
       "19            1\n",
       "826           1\n",
       "937           0\n",
       "846           1\n",
       "610           1\n",
       "819           0\n",
       "576           0\n",
       "847           1\n",
       "233           0\n",
       "595           0\n",
       "395           0\n",
       "46            1\n",
       "227           0\n",
       "911           1\n",
       "839           1\n",
       "407           0\n",
       "915           1\n",
       "540           1\n",
       "48            0\n",
       "428           0\n",
       "\n",
       "[757 rows x 1 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-fold cross validation:\n",
      "\n",
      "Accuracy: 0.88 (+/- 0.03) [KNN]\n",
      "Accuracy: 0.91 (+/- 0.03) [Random Forest]\n",
      "Accuracy: 0.82 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.89 (+/- 0.03) [StackingClassifier]\n",
      "Accuracy: 0.92 (+/- 0.03) [XGB]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "xg=XGBClassifier()\n",
    "\n",
    "print('3-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf,xg], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'StackingClassifier','XGB']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X_train, Y_train, \n",
    "                                              cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.010649283727010092\n",
      "fit_time  std  0.0024778156300469876\n",
      "score_time  mean  0.001329660415649414\n",
      "score_time  std  0.00047069604442453246\n",
      "test_score  mean  0.8983206809293766\n",
      "test_score  std  0.021340816458815946\n",
      "train_score  mean  0.933955419351459\n",
      "train_score  std  0.0060679631360644175\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.09778356552124023\n",
      "fit_time  std  0.01192775942777858\n",
      "score_time  mean  0.008017698923746744\n",
      "score_time  std  3.1541215150341724e-05\n",
      "test_score  mean  0.9062258192692975\n",
      "test_score  std  0.015866863374149484\n",
      "train_score  mean  0.9372557493844623\n",
      "train_score  std  0.0040196874538356575\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.02061144510904948\n",
      "fit_time  std  0.0009397631820553268\n",
      "score_time  mean  0.00864394505818685\n",
      "score_time  std  0.0016949834652245821\n",
      "test_score  mean  0.9062258192692975\n",
      "test_score  std  0.015866863374149484\n",
      "train_score  mean  0.9372557493844623\n",
      "train_score  std  0.0040196874538356575\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.004333813985188802\n",
      "fit_time  std  0.0004617890035223003\n",
      "score_time  mean  0.024269978205362957\n",
      "score_time  std  0.0004721021132045682\n",
      "test_score  mean  0.8784532697576175\n",
      "test_score  std  0.0076896402161784295\n",
      "train_score  mean  0.9273455917020273\n",
      "train_score  std  0.0018345192322813484\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.010649601618448893\n",
      "fit_time  std  0.00045244000235584574\n",
      "score_time  mean  0.0013314088185628254\n",
      "score_time  std  0.0004711465759641706\n",
      "test_score  mean  0.8586538260451304\n",
      "test_score  std  0.001752787418259177\n",
      "train_score  mean  0.9986785583320237\n",
      "train_score  std  0.0009344017410757877\n",
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.019955952962239582\n",
      "fit_time  std  0.0021459158769370746\n",
      "score_time  mean  0.002971013387044271\n",
      "score_time  std  1.596435487046524e-05\n",
      "test_score  mean  0.9048926114143505\n",
      "test_score  std  0.018012096812303872\n",
      "train_score  mean  0.9940580962858191\n",
      "train_score  std  0.00279857076424348\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.17413703600565592\n",
      "fit_time  std  0.010144373948170976\n",
      "score_time  mean  0.002003908157348633\n",
      "score_time  std  1.722343912313305e-05\n",
      "test_score  mean  0.9194324194324194\n",
      "test_score  std  0.01216370237268091\n",
      "train_score  mean  0.9900924616271151\n",
      "train_score  std  0.0016168514335584282\n",
      "---------------------------------\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "       min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "       n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "       objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "       reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "       tree_method=None, validate_parameters=False, verbosity=None)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.29209232330322266\n",
      "fit_time  std  0.008618547765542727\n",
      "score_time  mean  0.0023399988810221353\n",
      "score_time  std  0.0017077495532119783\n",
      "test_score  mean  0.9022837066315327\n",
      "test_score  std  0.020340577148756644\n",
      "train_score  mean  0.9980184923254231\n",
      "train_score  std  1.8521315448293845e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "clfs = []\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC(probability=True))\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(XGBClassifier())\n",
    "\n",
    "\n",
    "for classifier in clfs:\n",
    "    scores = cross_validate(classifier, X_train, Y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=700, objective='binary:logistic',silent=True, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=700, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957761351636748"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 7, 2)\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    7.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.187020 using {'max_depth': 5}\n",
      "-0.232401 (0.027072) with: {'max_depth': 1}\n",
      "-0.188893 (0.028963) with: {'max_depth': 3}\n",
      "-0.187020 (0.037774) with: {'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "max_depth = range(1, 7, 2)\n",
    "print(max_depth)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold, verbose=1)\n",
    "grid_result = grid_search.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(max_depth, means, yerr=stds)\n",
    "pyplot.title(\"XGBoost max_depth vs Log Loss\")\n",
    "pyplot.xlabel('max_depth')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('max_depth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=700, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "       importance_type='gain', interaction_constraints=None,\n",
    "       learning_rate=0.02, max_delta_step=0, max_depth=5,\n",
    "       min_child_weight=1, missing=None, monotone_constraints=None,\n",
    "       n_estimators=1000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
    "       tree_method=None, validate_parameters=False, verbosity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315789473684211"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 5, 6, 8, 15, 20, 25, 30, 35]\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done 467 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.183613 using {'max_depth': 4, 'n_estimators': 800}\n",
      "-0.442866 (0.021756) with: {'max_depth': 2, 'n_estimators': 50}\n",
      "-0.336940 (0.026358) with: {'max_depth': 2, 'n_estimators': 100}\n",
      "-0.287082 (0.026957) with: {'max_depth': 2, 'n_estimators': 150}\n",
      "-0.260006 (0.027631) with: {'max_depth': 2, 'n_estimators': 200}\n",
      "-0.233156 (0.028313) with: {'max_depth': 2, 'n_estimators': 300}\n",
      "-0.220735 (0.027891) with: {'max_depth': 2, 'n_estimators': 400}\n",
      "-0.213078 (0.029026) with: {'max_depth': 2, 'n_estimators': 500}\n",
      "-0.207155 (0.029676) with: {'max_depth': 2, 'n_estimators': 600}\n",
      "-0.202397 (0.029480) with: {'max_depth': 2, 'n_estimators': 700}\n",
      "-0.198954 (0.029750) with: {'max_depth': 2, 'n_estimators': 800}\n",
      "-0.393009 (0.026174) with: {'max_depth': 4, 'n_estimators': 50}\n",
      "-0.286965 (0.030977) with: {'max_depth': 4, 'n_estimators': 100}\n",
      "-0.245017 (0.031091) with: {'max_depth': 4, 'n_estimators': 150}\n",
      "-0.226766 (0.031641) with: {'max_depth': 4, 'n_estimators': 200}\n",
      "-0.208444 (0.030612) with: {'max_depth': 4, 'n_estimators': 300}\n",
      "-0.197743 (0.029190) with: {'max_depth': 4, 'n_estimators': 400}\n",
      "-0.191075 (0.027641) with: {'max_depth': 4, 'n_estimators': 500}\n",
      "-0.186943 (0.028575) with: {'max_depth': 4, 'n_estimators': 600}\n",
      "-0.185301 (0.029893) with: {'max_depth': 4, 'n_estimators': 700}\n",
      "-0.183613 (0.032357) with: {'max_depth': 4, 'n_estimators': 800}\n",
      "-0.385012 (0.026578) with: {'max_depth': 5, 'n_estimators': 50}\n",
      "-0.280418 (0.031687) with: {'max_depth': 5, 'n_estimators': 100}\n",
      "-0.241509 (0.031376) with: {'max_depth': 5, 'n_estimators': 150}\n",
      "-0.222866 (0.031291) with: {'max_depth': 5, 'n_estimators': 200}\n",
      "-0.204953 (0.031738) with: {'max_depth': 5, 'n_estimators': 300}\n",
      "-0.195167 (0.030759) with: {'max_depth': 5, 'n_estimators': 400}\n",
      "-0.188872 (0.032241) with: {'max_depth': 5, 'n_estimators': 500}\n",
      "-0.187756 (0.035839) with: {'max_depth': 5, 'n_estimators': 600}\n",
      "-0.187020 (0.037774) with: {'max_depth': 5, 'n_estimators': 700}\n",
      "-0.188132 (0.040458) with: {'max_depth': 5, 'n_estimators': 800}\n",
      "-0.385384 (0.025678) with: {'max_depth': 6, 'n_estimators': 50}\n",
      "-0.277246 (0.031350) with: {'max_depth': 6, 'n_estimators': 100}\n",
      "-0.238463 (0.032443) with: {'max_depth': 6, 'n_estimators': 150}\n",
      "-0.220690 (0.035419) with: {'max_depth': 6, 'n_estimators': 200}\n",
      "-0.204255 (0.037359) with: {'max_depth': 6, 'n_estimators': 300}\n",
      "-0.197196 (0.038067) with: {'max_depth': 6, 'n_estimators': 400}\n",
      "-0.196023 (0.040686) with: {'max_depth': 6, 'n_estimators': 500}\n",
      "-0.196532 (0.043657) with: {'max_depth': 6, 'n_estimators': 600}\n",
      "-0.198539 (0.045693) with: {'max_depth': 6, 'n_estimators': 700}\n",
      "-0.201373 (0.047504) with: {'max_depth': 6, 'n_estimators': 800}\n",
      "-0.385386 (0.028190) with: {'max_depth': 8, 'n_estimators': 50}\n",
      "-0.270857 (0.035856) with: {'max_depth': 8, 'n_estimators': 100}\n",
      "-0.228207 (0.036862) with: {'max_depth': 8, 'n_estimators': 150}\n",
      "-0.212592 (0.041433) with: {'max_depth': 8, 'n_estimators': 200}\n",
      "-0.200847 (0.046981) with: {'max_depth': 8, 'n_estimators': 300}\n",
      "-0.198070 (0.050147) with: {'max_depth': 8, 'n_estimators': 400}\n",
      "-0.199177 (0.052260) with: {'max_depth': 8, 'n_estimators': 500}\n",
      "-0.201973 (0.054354) with: {'max_depth': 8, 'n_estimators': 600}\n",
      "-0.205169 (0.056844) with: {'max_depth': 8, 'n_estimators': 700}\n",
      "-0.208566 (0.057914) with: {'max_depth': 8, 'n_estimators': 800}\n",
      "-0.388241 (0.029324) with: {'max_depth': 15, 'n_estimators': 50}\n",
      "-0.272816 (0.041407) with: {'max_depth': 15, 'n_estimators': 100}\n",
      "-0.230829 (0.044639) with: {'max_depth': 15, 'n_estimators': 150}\n",
      "-0.215733 (0.047520) with: {'max_depth': 15, 'n_estimators': 200}\n",
      "-0.207650 (0.053453) with: {'max_depth': 15, 'n_estimators': 300}\n",
      "-0.205851 (0.056738) with: {'max_depth': 15, 'n_estimators': 400}\n",
      "-0.207219 (0.057986) with: {'max_depth': 15, 'n_estimators': 500}\n",
      "-0.208878 (0.059285) with: {'max_depth': 15, 'n_estimators': 600}\n",
      "-0.211538 (0.060736) with: {'max_depth': 15, 'n_estimators': 700}\n",
      "-0.214587 (0.062416) with: {'max_depth': 15, 'n_estimators': 800}\n",
      "-0.388243 (0.029313) with: {'max_depth': 20, 'n_estimators': 50}\n",
      "-0.272828 (0.041338) with: {'max_depth': 20, 'n_estimators': 100}\n",
      "-0.230255 (0.044202) with: {'max_depth': 20, 'n_estimators': 150}\n",
      "-0.215581 (0.046985) with: {'max_depth': 20, 'n_estimators': 200}\n",
      "-0.207575 (0.052936) with: {'max_depth': 20, 'n_estimators': 300}\n",
      "-0.206062 (0.056903) with: {'max_depth': 20, 'n_estimators': 400}\n",
      "-0.207234 (0.057770) with: {'max_depth': 20, 'n_estimators': 500}\n",
      "-0.209881 (0.060195) with: {'max_depth': 20, 'n_estimators': 600}\n",
      "-0.212377 (0.061758) with: {'max_depth': 20, 'n_estimators': 700}\n",
      "-0.214918 (0.063377) with: {'max_depth': 20, 'n_estimators': 800}\n",
      "-0.388243 (0.029313) with: {'max_depth': 25, 'n_estimators': 50}\n",
      "-0.272828 (0.041338) with: {'max_depth': 25, 'n_estimators': 100}\n",
      "-0.230255 (0.044202) with: {'max_depth': 25, 'n_estimators': 150}\n",
      "-0.215581 (0.046985) with: {'max_depth': 25, 'n_estimators': 200}\n",
      "-0.207575 (0.052936) with: {'max_depth': 25, 'n_estimators': 300}\n",
      "-0.206062 (0.056903) with: {'max_depth': 25, 'n_estimators': 400}\n",
      "-0.207234 (0.057770) with: {'max_depth': 25, 'n_estimators': 500}\n",
      "-0.209881 (0.060195) with: {'max_depth': 25, 'n_estimators': 600}\n",
      "-0.212377 (0.061758) with: {'max_depth': 25, 'n_estimators': 700}\n",
      "-0.214918 (0.063377) with: {'max_depth': 25, 'n_estimators': 800}\n",
      "-0.388243 (0.029313) with: {'max_depth': 30, 'n_estimators': 50}\n",
      "-0.272828 (0.041338) with: {'max_depth': 30, 'n_estimators': 100}\n",
      "-0.230255 (0.044202) with: {'max_depth': 30, 'n_estimators': 150}\n",
      "-0.215581 (0.046985) with: {'max_depth': 30, 'n_estimators': 200}\n",
      "-0.207575 (0.052936) with: {'max_depth': 30, 'n_estimators': 300}\n",
      "-0.206062 (0.056903) with: {'max_depth': 30, 'n_estimators': 400}\n",
      "-0.207234 (0.057770) with: {'max_depth': 30, 'n_estimators': 500}\n",
      "-0.209881 (0.060195) with: {'max_depth': 30, 'n_estimators': 600}\n",
      "-0.212377 (0.061758) with: {'max_depth': 30, 'n_estimators': 700}\n",
      "-0.214918 (0.063377) with: {'max_depth': 30, 'n_estimators': 800}\n",
      "-0.388243 (0.029313) with: {'max_depth': 35, 'n_estimators': 50}\n",
      "-0.272828 (0.041338) with: {'max_depth': 35, 'n_estimators': 100}\n",
      "-0.230255 (0.044202) with: {'max_depth': 35, 'n_estimators': 150}\n",
      "-0.215581 (0.046985) with: {'max_depth': 35, 'n_estimators': 200}\n",
      "-0.207575 (0.052936) with: {'max_depth': 35, 'n_estimators': 300}\n",
      "-0.206062 (0.056903) with: {'max_depth': 35, 'n_estimators': 400}\n",
      "-0.207234 (0.057770) with: {'max_depth': 35, 'n_estimators': 500}\n",
      "-0.209881 (0.060195) with: {'max_depth': 35, 'n_estimators': 600}\n",
      "-0.212377 (0.061758) with: {'max_depth': 35, 'n_estimators': 700}\n",
      "-0.214918 (0.063377) with: {'max_depth': 35, 'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [50, 100, 150, 200,300,400,500,600,700,800]\n",
    "max_depth = [2, 4, 5,6, 8,15,20,25,30,35]\n",
    "print(max_depth)\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(xgb, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold, verbose=1)\n",
    "grid_result = grid_search.fit(X_train, Y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot results\n",
    "scores = np.array(means).reshape(len(max_depth), len(n_estimators))\n",
    "for i, value in enumerate(max_depth):\n",
    "    pyplot.plot(n_estimators, scores[i], label='depth: ' + str(value))\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('n_estimators_vs_max_depth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'n_estimators': 800}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "       importance_type='gain', interaction_constraints=None,\n",
    "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, monotone_constraints=None,\n",
    "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
    "       tree_method=None, validate_parameters=False, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926082365364308"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947368421052631"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92154 (+/- 0.00) [KNN]\n",
      "Accuracy: 0.97179 (+/- 0.01) [Random Forest]\n",
      "Accuracy: 0.97559 (+/- 0.00) [Grad]\n",
      "Accuracy: 0.96109 (+/- 0.01) [ada]\n",
      "Accuracy: 0.97836 (+/- 0.00) [xgb]\n",
      "Accuracy: 0.97871 (+/- 0.00) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2 = RandomForestClassifier(random_state=1, n_estimators=300)\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5=AdaBoostClassifier()\n",
    "# Logit will be used for stacking\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "xgb = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "       importance_type='gain', interaction_constraints=None,\n",
    "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, monotone_constraints=None,\n",
    "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
    "       tree_method=None, validate_parameters=False, verbosity=None)\n",
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf4,clf5,xgb], meta_classifier=lr, use_probas=True, cv=3)\n",
    "\n",
    "# Do CV\n",
    "for clf, label in zip([clf1, clf2, clf4,clf5,xgb,sclf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Grad',\n",
    "                       'ada',\n",
    "                       'xgb',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, scaler_train, Y_train, cv=3, scoring='roc_auc')\n",
    "    print(\"Accuracy: %0.5f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "# Fit on train data / predict on test data\n",
    "sclf_fit = sclf.fit(scaler_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[116   0]\n",
      " [  1  73]]\n",
      "Accuracy Score : 0.9947368421052631\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       116\n",
      "           1       1.00      0.99      0.99        74\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       190\n",
      "   macro avg       1.00      0.99      0.99       190\n",
      "weighted avg       0.99      0.99      0.99       190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "results = confusion_matrix(np.array(Y_test).flatten(), pred) \n",
    "print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(np.array(Y_test).flatten(), pred) )\n",
    "\n",
    "print(classification_report(np.array(Y_test).flatten(), pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler=MinMaxScaler()\n",
    "scaler=StandardScaler()\n",
    "scaler_train=mmscaler.fit_transform(X_train)\n",
    "scaler_test=mmscaler.transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 53)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527, 53)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "       importance_type='gain', interaction_constraints=None,\n",
    "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
    "       min_child_weight=1, missing=None, monotone_constraints=None,\n",
    "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
    "       tree_method=None, validate_parameters=False, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=800, n_jobs=1, nthread=1, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1,\n",
       "       tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(scaler_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9926082365364308"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.score(scaler_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict(scaler_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'IsGoodNews':xgb.predict(scaler_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_Of_Word_1</th>\n",
       "      <th>Freq_Of_Word_2</th>\n",
       "      <th>Freq_Of_Word_3</th>\n",
       "      <th>Freq_Of_Word_4</th>\n",
       "      <th>Freq_Of_Word_5</th>\n",
       "      <th>Freq_Of_Word_6</th>\n",
       "      <th>Freq_Of_Word_7</th>\n",
       "      <th>Freq_Of_Word_8</th>\n",
       "      <th>Freq_Of_Word_9</th>\n",
       "      <th>Freq_Of_Word_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_Of_Word_45</th>\n",
       "      <th>Freq_Of_Word_46</th>\n",
       "      <th>Freq_Of_Word_47</th>\n",
       "      <th>Freq_Of_Word_48</th>\n",
       "      <th>Freq_Of_Word_49</th>\n",
       "      <th>Freq_Of_Word_50</th>\n",
       "      <th>TotalEmojiCharacters</th>\n",
       "      <th>LengthOFFirstParagraph</th>\n",
       "      <th>StylizedLetters</th>\n",
       "      <th>IsGoodNews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>2.620660</td>\n",
       "      <td>1.253645</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>0.079303</td>\n",
       "      <td>0.157385</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>0.222453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>3.837751</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.195476</td>\n",
       "      <td>-0.408024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>-0.453742</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.187634</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.210190</td>\n",
       "      <td>2.682528</td>\n",
       "      <td>1.291868</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>0.221744</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>0.859101</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>2.374782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.340723</td>\n",
       "      <td>1.334201</td>\n",
       "      <td>2.270899</td>\n",
       "      <td>0.602985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.351864</td>\n",
       "      <td>-0.318036</td>\n",
       "      <td>-0.561952</td>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.465210</td>\n",
       "      <td>-0.353977</td>\n",
       "      <td>-0.304257</td>\n",
       "      <td>-0.240708</td>\n",
       "      <td>-0.318797</td>\n",
       "      <td>-0.352968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.323019</td>\n",
       "      <td>-0.205212</td>\n",
       "      <td>-0.079531</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.151911</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>-0.028751</td>\n",
       "      <td>-0.113133</td>\n",
       "      <td>-0.128592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Freq_Of_Word_1  Freq_Of_Word_2  Freq_Of_Word_3  Freq_Of_Word_4  \\\n",
       "0       -0.351864        2.620660        1.253645       -0.039223   \n",
       "1       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "2       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "3        1.210190        2.682528        1.291868       -0.039223   \n",
       "4       -0.351864       -0.318036       -0.561952       -0.039223   \n",
       "\n",
       "   Freq_Of_Word_5  Freq_Of_Word_6  Freq_Of_Word_7  Freq_Of_Word_8  \\\n",
       "0       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "1       -0.465210       -0.353977       -0.304257        3.837751   \n",
       "2       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "3        0.221744       -0.353977       -0.304257        0.859101   \n",
       "4       -0.465210       -0.353977       -0.304257       -0.240708   \n",
       "\n",
       "   Freq_Of_Word_9  Freq_Of_Word_10  ...  Freq_Of_Word_45  Freq_Of_Word_46  \\\n",
       "0       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "1       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "2       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "3       -0.318797         2.374782  ...        -0.323019        -0.205212   \n",
       "4       -0.318797        -0.352968  ...        -0.323019        -0.205212   \n",
       "\n",
       "   Freq_Of_Word_47  Freq_Of_Word_48  Freq_Of_Word_49  Freq_Of_Word_50  \\\n",
       "0        -0.079531        -0.118688         0.079303         0.157385   \n",
       "1        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "2        -0.079531        -0.118688        -0.151911        -0.453742   \n",
       "3        -0.079531        -0.118688        -0.151911         0.340723   \n",
       "4        -0.079531        -0.118688        -0.151911         0.930461   \n",
       "\n",
       "   TotalEmojiCharacters  LengthOFFirstParagraph  StylizedLetters  IsGoodNews  \n",
       "0             -0.028751               -0.046474         0.222453           1  \n",
       "1             -0.107383               -0.195476        -0.408024           0  \n",
       "2             -0.107383               -0.187634        -0.392578           0  \n",
       "3              1.334201                2.270899         0.602985           1  \n",
       "4             -0.028751               -0.113133        -0.128592           0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Sample_Submission.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97836 (+/- 0.00) \n"
     ]
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(xgb, scaler_train,Y_train, cv=3, scoring='roc_auc')\n",
    "print(\"Accuracy: %0.5f (+/- %0.2f) \" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
